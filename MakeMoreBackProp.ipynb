{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNW9s/9rWm0iWENwISuArqc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samin-Sadaf7/NN_works/blob/main/MakeMoreBackProp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "SHOyS8VCoCWL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the names.txt file from github\n",
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhJiJ61RogZQ",
        "outputId": "ba00ac7e-fa85-4f80-89fa-507a84b0cbb0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-18 08:50:34--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "names.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-08-18 08:50:34 (6.43 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHUjNwaQo32s",
        "outputId": "803e7fe2-ce00-466c-942c-1fd7fb7fd5ff"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "id": "nac5LTHWo6S4",
        "outputId": "8a53c9e8-33c5-4984-b6a5-5e7064aebd66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ],
      "metadata": {
        "id": "eFaThhBGo8pn",
        "outputId": "69d92729-5a14-4797-90de-f27d4e09b968",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "3MyL-iMCo_jo"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHPmO6lI54UC",
        "outputId": "c826047f-853d-47e9-ee8f-26393d767e31"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ],
      "metadata": {
        "id": "e9npC6YG7Bma"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "\n",
        "\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "\n",
        "\n",
        "\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8O7xcgz7b9N",
        "outputId": "224ebd90-4cc4-4a01-d4af-91c64db2eabf"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3436, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39FZz1d88ZOW",
        "outputId": "f3c939c9-ecb9-40c8-95bd-2bb5de20f0ac"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 27])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "cmp('logprobs', dlogprobs, logprobs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-mUoaFLAtIj",
        "outputId": "d7d5f9bc-0cca-46a7-db18-2408b37508f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dprobs = torch.zeros_like(probs)\n",
        "dprobs = (1.0/probs)* dlogprobs\n",
        "cmp('probs', dprobs, probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ-V77aVGl8B",
        "outputId": "4249cad0-39ee-4f5c-c444-cd32e79a14dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim = True)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP1R_bdbKTZv",
        "outputId": "ee1cd091-5b1a-4624-ab03-1742582dcb6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnNSprPoMws7",
        "outputId": "b5316a41-344e-4bc1-984d-e93e0bb16424"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dcounts = dprobs* counts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "cmp('counts', dcounts, counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyW-DRYjMJC7",
        "outputId": "5448920f-a184-4e85-ecb7-b422090bbc07"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnorm_logits = counts * dcounts #counts = normlogits.exp()\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E2rP3k9MXZ-",
        "outputId": "b068b5ec-b49f-4437-b332-5d2183b1d284"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qp6602TCQ7UX",
        "outputId": "1c4bc190-11ac-42ae-be5e-302671c483a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "cmp('logits', dlogits, logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDZXetjBZkMD",
        "outputId": "f75e4c4b-4c24-4a34-d4e6-b3d19ddef07c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dh = dlogits @ W2.T\n",
        "cmp('h', dh, h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub9HhsTyZzsx",
        "outputId": "6c4f3703-c5f2-442a-803e-32f9967ace91"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dW2 = h.T @ dlogits\n",
        "cmp('W2', dW2, W2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTIBAcXvkmWA",
        "outputId": "02bf2418-7d00-4abd-c9ab-2c9cb692dea0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db2 = dlogits.sum(0)\n",
        "cmp('b2', db2, b2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztYonpDmk8V0",
        "outputId": "2a35c4e8-24ac-4af0-e9cd-3d7d4ce263ed"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dhpreact = (1.0 - h**2) * dh\n",
        "cmp('hpreact', dhpreact, hpreact)"
      ],
      "metadata": {
        "id": "MocbGX7rlIzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7fd4d35-7b08-4bb5-fea7-2cbee2ff5b13"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hpreact         | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hpreact.shape, bngain.shape, bnraw.shape, bnbias.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjcrMwEliy5T",
        "outputId": "f7f913c1-82ee-4c47-a576-5292421d5d58"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([1, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "cmp('bngain', dbngain, bngain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXL6jV-ikEwu",
        "outputId": "f624e90c-77b8-4aef-db03-c6bdea5901fe"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbnraw = bngain * dhpreact\n",
        "cmp('bnraw', dbnraw, bnraw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58jWTtWYkgc8",
        "outputId": "b18a3ca2-54c3-465f-c807-3581fa8ea46a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbnbias = dhpreact.sum(0, keepdims=True)\n",
        "cmp('bnbias', dbnbias, bnbias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgDlG5c7lEIG",
        "outputId": "f6dea880-c9e7-41c7-caec-078cfdd8feaf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bndiff.shape, bnraw.shape, bnvar_inv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTVWz_Vfkr6O",
        "outputId": "700ee659-91b5-4c51-cde4-f88241dad8f1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([32, 64]), torch.Size([1, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbndiff = bnvar_inv * dbnraw\n",
        "cmp('bndiff', dbndiff, bndiff)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvfGgZMsk9mi",
        "outputId": "df1081ad-e456-4131-c159-effcd90c594a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bndiff          | exact: False | approximate: False | maxdiff: 0.0010384263005107641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-t3cJ0xlg-f",
        "outputId": "3b95bd12-a851-42ee-d4a7-a3faa1e13ad6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnvar.shape, bndiff2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhyoEC-LmHhZ",
        "outputId": "72037ba7-0837-4708-e367-3dcc48dce9e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 64]), torch.Size([32, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "cmp('bnvar', dbnvar, bnvar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e42QTL35m47l",
        "outputId": "ccf1f5c9-d714-4156-8ca6-ab52a7bf3515"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnvar           | exact: False | approximate: True  | maxdiff: 5.238689482212067e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "cmp('bndiff2', dbndiff2, bndiff2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqM5dKpfnJr2",
        "outputId": "0c9b5655-d5cd-4710-c472-042c110e122f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bndiff2         | exact: False | approximate: True  | maxdiff: 1.8189894035458565e-11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbndiff += 2* bndiff * dbndiff2\n",
        "cmp('bndiff', dbndiff, bndiff)"
      ],
      "metadata": {
        "id": "BGYSN7SSnQd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d98c7316-94f6-4a51-8af2-9e3b2e7edc60"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bndiff = hprebn - bnmeani\n",
        "hprebn.shape, bnmeani.shape, bndiff.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phVKIuT76w3z",
        "outputId": "12ecc2a9-2a49-425b-ccfa-4e3734b5c2b3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]), torch.Size([1, 64]), torch.Size([32, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dbnmeani = -dbndiff.sum(0)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ9tfGcj7sCf",
        "outputId": "590bdf12-9c7e-46ae-fe60-e18a0c542057"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bnmeani         | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dhprebn = dbndiff.clone()\n",
        "dhprebn += (1.0/n) * (torch.ones_like(hprebn) * dbnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jeca_PhJ8Aui",
        "outputId": "bde6dbc7-0a6e-4041-d11f-70eaaa76271e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "embcat.shape, W1.shape, b1.shape, hprebn.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAJTVvDp9CHS",
        "outputId": "ea29a95b-3bb9-47fb-86ff-d5e380260712"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 30]),\n",
              " torch.Size([30, 64]),\n",
              " torch.Size([64]),\n",
              " torch.Size([32, 64]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dembcat = dhprebn @ W1.T\n",
        "cmp('embcat', dembcat, embcat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8DnYzpu_gMa",
        "outputId": "deb1034b-e87a-4c6e-b094-64b0d76f1c00"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dW1 = embcat.T @ dhprebn\n",
        "cmp('W1', dW1, W1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRPtMlD9_sK4",
        "outputId": "9f889c1c-76da-439b-a7e5-e6aada0460a8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1              | exact: False | approximate: True  | maxdiff: 4.6566128730773926e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db1 = dhprebn.sum(0)\n",
        "cmp('b1', db1, b1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRCaXNA7_wfX",
        "outputId": "1362b0df-c4d1-4fef-df4b-193684870974"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b1              | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#emb = C[Xb] # embed the characters into vectors\n",
        "#embcat = emb.view(emb.shape[0], -1) # concatenate the vectors"
      ],
      "metadata": {
        "id": "Y3YdlDXP_zKj"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demb = dembcat.view(emb.shape)\n",
        "cmp('emb', demb, emb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvNu7pcsAblG",
        "outputId": "8f7ae157-03f0-4993-b0b6-e5d4f37392f8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Xb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMNCmmLYDDaS",
        "outputId": "15003f12-3db5-49f7-85af-e24883587671"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bTxMNbPAfVf",
        "outputId": "3341e978-042f-45bc-8b89-c16bd524b27c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C               | exact: False | approximate: True  | maxdiff: 8.381903171539307e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfCO_CYlBpnQ",
        "outputId": "f127b98a-6d96-4974-a8e1-5d72cad0cf93"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 1.4551915228366852e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 2.3865140974521637e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "id": "v0XuBcTXBTLb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11eefe20-fa8a-4534-f366-b36a9dedea3f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.3435935974121094 diff: -2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "cmp('logits', dlogits, logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_yOOFgHr67G",
        "outputId": "ff7cc56c-6600-4ff1-ba7b-4601bf159930"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 7.2177499532699585e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits.shape, Yb.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-KoKOF5BVfz",
        "outputId": "e9eb3a19-c07d-4141-ab06-419ef8b4ab9b"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(logits, 1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dy952I6FBX2u",
        "outputId": "342d6063-5d5c-47f0-a25f-2308ba4454aa"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0689, 0.0887, 0.0189, 0.0447, 0.0189, 0.0846, 0.0232, 0.0378, 0.0204,\n",
              "        0.0300, 0.0374, 0.0363, 0.0381, 0.0269, 0.0297, 0.0133, 0.0086, 0.0207,\n",
              "        0.0164, 0.0592, 0.0493, 0.0238, 0.0286, 0.0685, 0.0581, 0.0274, 0.0217],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0] * n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPz8pUOCBaMH",
        "outputId": "9cc04ee2-539a-42c1-d5af-a08114bab64a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0689,  0.0887,  0.0189,  0.0447,  0.0189,  0.0846,  0.0232,  0.0378,\n",
              "        -0.9796,  0.0300,  0.0374,  0.0363,  0.0381,  0.0269,  0.0297,  0.0133,\n",
              "         0.0086,  0.0207,  0.0164,  0.0592,  0.0493,  0.0238,  0.0286,  0.0685,\n",
              "         0.0581,  0.0274,  0.0217], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0].sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVhQIyaBcfN",
        "outputId": "8bb7cc27-9bbc-489c-dba4-4fea15aba4b4"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5611e-09, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "xq5AEnxmBgDx",
        "outputId": "68cb813c-1317-4f8c-c5d4-a4393cac4372"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7a921a20ba90>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "# BatchNorm paper: https://arxiv.org/abs/1502.03167\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmvSyGXGsBB-",
        "outputId": "a39b9617-d205-4441-961d-b936d18c351c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "# -----------------\n",
        "# YOUR CODE HERE :)\n",
        "dhprebn = bngain* bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcOg8ojnsXdG",
        "outputId": "a3961bf8-ef25-4aea-aff6-16184a08f2de"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "  #     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1TPcMvesciG",
        "outputId": "044a9996-30dd-4ac1-833b-beaaf99bdc37"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.8105\n",
            "  10000/ 200000: 2.1842\n",
            "  20000/ 200000: 2.3262\n",
            "  30000/ 200000: 2.4432\n",
            "  40000/ 200000: 2.0049\n",
            "  50000/ 200000: 2.3470\n",
            "  60000/ 200000: 2.3330\n",
            "  70000/ 200000: 2.0241\n",
            "  80000/ 200000: 2.3704\n",
            "  90000/ 200000: 2.1450\n",
            " 100000/ 200000: 2.0077\n",
            " 110000/ 200000: 2.4194\n",
            " 120000/ 200000: 2.0341\n",
            " 130000/ 200000: 2.5302\n",
            " 140000/ 200000: 2.2741\n",
            " 150000/ 200000: 2.1171\n",
            " 160000/ 200000: 1.9734\n",
            " 170000/ 200000: 1.8223\n",
            " 180000/ 200000: 1.9731\n",
            " 190000/ 200000: 1.9084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "id": "hxXUR38esg1d"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "WZzzzqWRsi8v"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4Fis3SaslZu",
        "outputId": "170e273b-0072-4fb0-e4ff-4639871ba291"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0723159313201904\n",
            "val 2.1098828315734863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ],
      "metadata": {
        "id": "HCeml8Ihsnl9"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      if ix == 0:\n",
        "        break\n",
        "      out.append(ix)\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YOwsv4nsqTQ",
        "outputId": "17fc51bf-f9b4-4c63-e536-ea0c8575f754"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora\n",
            "mayah\n",
            "see\n",
            "madhayla\n",
            "ren\n",
            "ruthadrie\n",
            "cailee\n",
            "melin\n",
            "shi\n",
            "jen\n",
            "eden\n",
            "sana\n",
            "arleigh\n",
            "malaia\n",
            "noshuberlihira\n",
            "sten\n",
            "joselle\n",
            "joseus\n",
            "kuba\n",
            "geder\n"
          ]
        }
      ]
    }
  ]
}